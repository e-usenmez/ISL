---
title: "ISL Ch 3: Linear Regression: Applied"
author: Emre Usenmez
date: 2021-Jan
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    number_sections: false
    theme: simplex
---


# 8. Simple LR
This question involves the use of simple linear regression on the Auto data set.

## 8.(a) 
Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.
```{r}
Auto <- read.csv('~/Documents/Programming/Data Science/Book-ISLR/Data/Auto.csv', header = TRUE, na.strings = '?')
Auto <- na.omit(Auto)
auto.lm <- lm(mpg ~ horsepower, data = Auto)
summary(auto.lm)
```
### 8.(a)i. 
Is there a relationship b/w the predictor and the response?

>> For this we need to test whether the coefficient of horsepower is different than 0. (ie. $H_0 : \beta_1 = 0$). To do so we look at the t-statistic where $t = \frac {\hat\beta_1-0}{SE(\hat\beta_1)}$ which measures the number of st.devs that $\hat\beta_1$ is away from 0. If there is really no relationship between X and Y, then we expect that the t-statistic will have a t-distribution with n-2 d.f.
<br><br>
Once we know our t-statistic we can then look at the p-value which is the probability of observing a value equal to or greater than |t| if $H_0$ is true. The lower the probability, the higher the chance that the coefficient is different from 0.
<br><br>
The t-statistic in this example is -24.49 and the probability of observing any value greater than or equal to 24.49 is virtually 0 (ie the p-value < 2.2e-16). Therefore, we can reject the null hypothesis and declare a relationship between horsepower and mpg.
<br><br>
*Additional info: To determine the extent to which the model fits the data, we need the RSE and the R-squared. (No need for an F-test since there is only 1 predictor)
RSE is considered a measure of the lack of fit of the model to the data. The summary table tells us it is 4.906. This can be interpreted as mpg for each vehicle deviate from the true regression line on average by approximately 4.906. Whether this is an acceptable prediction error is a different issue. This is why R-squared can be a better tool, since it is not measured in the units of mpg. Accordingly, we see that this model explains about 60.59% of the variance in mpg.*

### 8.(a)ii. 
How strong is the relationship between the predictor and the response?

>> For every unit increase in horsepower, we expect on average a decline of 0.157845. So with an increase of 100 horsepower we would expect a drop of about 15.8 mpg.

### 8.(a)iii.
Is the relationship between the predictor and the response positive or negative?

>> Negative

### 8.(a)iv. 
What is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals?

>> We can use the predict() function we learned in the Ch 3 Lab.

```{r}
predict(auto.lm, data.frame(horsepower=98),interval = 'confidence')
```
>> and

```{r}
predict(auto.lm, data.frame(horsepower=98),interval = 'prediction')
```
>> Both intervals center around the associated mpg of 24.46708 but as expected the prediction interval is much wider than the confidence interval. 

<br><br>

## 8.(b) 
Plot the response and the predictor. Use the abline() function to display the least squares regression line.
```{r}
plot(Auto$horsepower,Auto$mpg)
abline(auto.lm)
```

<br><br>

## 8.(c) 
Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.
```{r}
par(mfrow=c(2,2))
plot(auto.lm)
```

> Section 3.3.3. discusses the diagnostics (the Lab notes I posted also reproduce the key points).
From the top left residuals graph we can immediately see the non-linearity of this data (since there is a clear pattern in the residuals, in this case its a bit like a U-shape) and heteroskedasticity (since there is a conical pattern).
<br><br>
The bottom right graph indicates a high leverage point at 117. Lets look at it closer:

```{r}
plot(hatvalues(auto.lm))
which.max(hatvalues(auto.lm))
```
> So it seems that data points 116 and 117 show high leverage that could be having a strong effect (ie leverage) on the model. 

<br><br><br><br><br><br>




# 9. Multiple LR & Diagnostics
This question involves the use of multiple linear regression on the Auto data set. Produce a scatterplot matrix which includes all of the variables in the data set.

## 9.(a) 
Produce a scatterplot matrix which includes all of the variables in the data set.
```{r}
pairs(Auto)
```

<br><br>

## 9.(b) 
Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.
```{r}
auto.corr <- cor(Auto[,1:8])
auto.corr
```

> The following is not shown in the book thus far but I saw this elsewhere and I thought it was pretty useful. (You will need to first install the corrplot package, if you don't have it already).

```{r}
library(corrplot)
corrplot(auto.corr)
```

> This plot gives a good visual indication of the correlations. Correlations range from dark red (-1) to dark blue (+1), and the sizes of the circles show the magnitudes of the correlations.

<br><br>

## 9.(c) 
Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output.

> *(Note: I kept getting the following error when I tried lm(mpg ~., -name, data=Auto):*
*"Error in 'contrasts<-'(...value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels.*<br><br> 
*It took me a while to figure out what was wrong and it turned out the problem the comma after '~.' which resulted in R naturally not recognizing the '-' sign when running the regression. Of course, the correct format should be without that ',' as below.)*

```{r}
auto.mullm <- lm(mpg ~ . - name, data = Auto)
summary(auto.mullm)
```

### 9.(c)i. 
Is there a relationship between the predictors and the response?

>> For the simple linear regression (eg, mpg vs horsepower above) we checked whether $\beta_1 = 0$ or not. In a multiple regression, we check if all the variables are different from 0 to find out if there is a relationship between the predictors and the response. Accoringly, the null and alternative hyopthesis are:
<br><br>
$H_0 : \beta_1 = \beta_2 = ...=\beta_p = 0$ and $H_a :$ at least one $\beta_j$ is non-zero.
<br><br>
We use F-statistic to test for this, where $F = \frac {\frac {(TSS-RSS)}{p}}{\frac {RSS}{(n-p-1)}}$
<br><br>
To be able to reject the null we would look for an F-stat greater than 1. If the F-stat is close to 1 we would fail to reject the null (though how close to 1 depends on the values of n and p. See Section 3.2.2 for details).
<br><br>
In this regression the F-stat is 252.4, and the probability of observing any value greater than or equal to F-stat is virtually 0 (ie the p-value < 2.2e-16) so we can comfortably reject the null and conclude that there is a relationship between at least one of the predictors and mpg.

### 9.(c)ii. 
Which predictors appear to have a statistically significant relationship to the response?

>> Based on the p-values of each of the predictors, we can see that for cylinders, horsepower and acceleration we cannot reject the null hypothesis that they are different from 0. For weight, year and origin we can reject the null and state that there is a likely statistically significant relationship. Finally, the null hypothesis for displacement can be rejected at p=0.01 but not at p=0.005.

### 9.(c)iii. 
What does the coefficient for the year variable suggest?

>> The coefficient for the year indicates that, holding all else the same, 1 year increase (ie as the cars get younger), on average, translates into an additional 0.75 miles per gallon.

<br><br>

## 9.(d)
Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
```{r}
par(mfrow=c(2,2))
plot(auto.mullm)
```

> Off the bat we can identify the presence of some non-linearity and heteroskedasticity from the top left graph. We can also see a high leverage at point 14 in the bottom right graph. But it is not an outlier since its within -2 standardized residuals.
We can also check the leverage by plotting the hat values and get the largest hat value.

```{r}
plot(hatvalues(auto.mullm))
which.max(hatvalues(auto.mullm))
```

<br><br><br>

## 9.(e) 
Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?

> Lets try all the interaction effects together.

```{r}
auto.interactlm <- lm(mpg ~ . -name + cylinders*displacement  + cylinders*horsepower + cylinders*weight + cylinders*acceleration + cylinders*year + cylinders*origin + displacement*horsepower + displacement*weight + displacement*acceleration + displacement*year + displacement*origin + horsepower*weight + horsepower*acceleration + horsepower*year + horsepower*origin + weight*acceleration + weight*year + weight*origin + acceleration*year + acceleration*origin + year*origin, data = Auto)
summary(auto.interactlm)
```

> Let's then compare the adjusted R-squares of the two models. (We are comparing the *adjusted* R-squares because the regular R-squared will increase simply by adding more variables.) The adjusted R-squared is 88.08% with the interaction terms compared to 81.82% without the interaction terms. This means:

```{r}
(88.08-81.82)/(100-81.82)
```
> about 34.4% of variability in mpg remaining after fitting the additive model (ie without the interactions) is explained by the interactions.
<br><br>
Of course not all the interactions are statistically significant. In fact acceleration:origin is the only one that is significant at p=0.01. acceleration:year and displacement:year are significant at 0.05, and cylinders:acceleration, cylinders:year, displacement:weight, horsepower:acceleration and year:origin are significant only at p=0.1.
<br><br>
Also note the presence of multicollinearity as identified in the correlation calculation and plot above (in part (b))
<br><br>
To improve the model, we can use the mixed selection method (see Section 3.2.2.) for variable selection to improve the model fit.

<br><br>

## 9.(f) 
Try a few different transformations of the variables, such as log(X), √X, X2. Comment on your findings.

> Looking at the pair plots (in part (a)) displacement and horsepower are showing clear non-linearity with mpg. weight seem to also show a potential non-linearity though less clearly. So let's try to transform these:

```{r}
auto.loglm <- lm(mpg ~ . - name + log(displacement) + log(horsepower) +log(weight), data = Auto)
summary(auto.loglm)
```

> The log transformed model's adjusted R-squared is 86.07% compared to 81.82% of the additive model. This means:

```{r}
(86.07-81.82)/(100-81.82)
```
> 23.38% of the variability in mpg remaining after fitting the additive model is explained by the log transformations. This is not as strong as the interaction terms (which is expected as there is clear presence of multicollinearity) though still an improvement on the additive model.
<br><br>
We see that the coefficient of the horsepower log transformation is significantly different from 0 while this is also true for the log transformation of weight, albeit only at p=0.05 level.
<br><br>
Lets check the square root transformation next:

```{r}
auto.sqrtlm <- lm(mpg ~ . - name + sqrt(displacement) + sqrt(horsepower) +sqrt(weight), data = Auto)
summary(auto.sqrtlm)
```
> There is a marginal improvement over the log model with the adjusted R-squared at 86.14%. The significance of the transformed variables is pretty much the same as the log transformations.
<br><br>
Finally, lets look at the squared transformations:

```{r}
auto.sqrlm <- lm(mpg ~ . - name + I(displacement^2) + I(horsepower^2) + I(weight^2), data = Auto)
summary(auto.sqrlm)
```
>This model is also a slight improvement on both log and sqrt transformations with the adusted R-square of 86.18%.
<br><br>
In addition to the statistically significant variables in the other models, in this model weight variable is statistically significant and its squared transformation is also statistically significant at p=0.01. Similarly, displacement and and its squared transformation are statistically significant only at p=0.1.

<br><br><br><br><br><br>


# 10. Multiple LR & Prediction
This question should be answered using the Carseats data set. (It is within the ISLR library.)
```{r}
library(ISLR)
?Carseats
fix(Carseats)
```

<br><br>

## 10.(a) 
Fit a multiple regression model to predict Sales using Price, Urban, and US.

```{r}
carseats.lm <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(carseats.lm)
```

<br><br>

## 10.(b) 
Provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!

> Price for car seats has a negative statistically significant relationship with carseat sales. For each unit increase in the price, the sales decline on average by about 54.5 car seats.
<br><br>
R created 2 dummy variables, UrbanYes and USYes.

```{r}
contrasts(Carseats$Urban)
contrasts(Carseats$US)
```
> The former takes a value of 1 if the store is in an urban area and 0 if in a rural location. The latter takes a value of 1 if the store is in the US and 0 if not.
<br><br>
The negative sign of UrbanYes indicate that a store in an urban location is associated with lower sales compared to a store in a rural location. However, it is not statistically significant, ie we can't reject the null that this coefficient is different than 0.
<br><br>
The positive sign for USYes indicate tht a store located in the US is associated with higher sales compared to non-US stores, with, on averge, an additional 1200.6 seats. This is statistically significant.

<br><br>

## 10.(c) 
Write out the model in equation form, being careful to handle the qualitative variables properly.

> $\hat{Sales}$ = 13.044 + (-0.55xPrice) + (-0.022xUrban) + (1.2xUS) where Urban and US are dummy variables encoded as Urban = 0 if a store is in a rural area and US = 0 if a store is not in the US.

<br><br>

## 10.(d)
For which of the predictors can you reject the null hypothesis $H_0 : \beta_j = 0$?

> We can reject the null for Price and US predictors.

<br><br>

## 10.(e)
On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.
```{r}
carseats.smlm <- lm(Sales ~ Price + US, data = Carseats)
summary(carseats.smlm)
```

<br><br>

## 10.(f) 
How well do the models in (a) and (e) fit the data?

>There is a marginal improvement over the model that includes the Urban variable. While the model in (a) explains 23.35% of the varibility in car seat sales, the model without the Urban variable can explain 23.54% of that variability. 

<br><br>

## 10.(g) 
Using the model from (e), obtain 95% confidence intervals for the coefficient(s).
```{r}
confint(carseats.smlm)
```

<br><br>

## 10.(h) 
Is there evidence of outliers or high leverage observations in the model from (e)?

```{r}
par(mfrow=c(2,2))
plot(carseats.smlm)
```

> Based on the lower-right graph, we can see that all residuals are within -3 and 3, so there aren't any outliers but there is a data point with high leverage.

<br><br><br><br><br><br>


# 11. Simple LR (w/o Intercept) & Significance
In this problem we will investigate the t-statistic for the null hypothesis $H_0 : \beta = 0$ in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.

```{r}
set.seed(1)
x=rnorm(100)
y=2*x+rnorm(100)
```

<br><br>

## 11.(a) 
Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate $\hat\beta$, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis $H_0 : \beta = 0$. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).)
```{r}
eleven.lm <- lm(y ~ x + 0)
summary(eleven.lm)
```
> The coefficient estimate is 1.9939 with a standard error of 0.1065, t-stat of 18.73 and a p-value - which is the prob. of observing that value equal to or greater than this t-stat if $H_0$ is true - of effectively 0. So we can conclude that there is a statistically significant relationship between x and y, whereby a unit increase in x results, on average, in an increase of 1.9939 in y, with the regression line passing through (0, 0). The standard error tells us that this estimate differs from the actual value by 0.1065 on average.

<br><br>

## 11.(b) 
Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis $H_0 : \beta = 0$. Comment on these results.
```{r}
elevenb.lm <- lm(x ~ y + 0)
summary(elevenb.lm)
```

> The coefficient estimate is 0.39111 with a std. err. of 0.02089, t-stat of 18.73 and a p-value of 0. As before, we can conclude that there is a statistically significant relationship between the predictor and the response, whereby a unit increase in y corresponds to a 0.39111 increase in x on average, and that this estimate differs from the actual value on average by 0.02089.

<br><br>

## 11.(c) 
What is the relationship between the results obtained in (a) and (b)?

> In both models R-squared and the t-stats (thus their p-values too) are the same. This is expected as the models are inverse of each other and since they are the only variable in explaining the other, they ought to explain the same amount of variability in each other. Similarly, and for the same reasons, the t-stat that measures the number of st.devs the variable is away from 0 would have to be the same too.

<br><br>

## 11.(d) 
Fro the regression of Y onto X without an intercept, the t-statistic for $H_0 : \beta = 0$ takes the form $\frac{\hat\beta}{SE(\hat\beta)}$, where $\hat\beta$ is given by (3.38), and where 

<div align='center'>
$SE(\hat\beta) = \sqrt\frac{\sum_{i=1}^n(y_i-x_i\hat\beta)^2}{(n-1)\sum_{i^{'}=1}^nx_{i^{'}}^2}$
</div>
(These formulas are slightly different from those given in Sections 3.1.1 and 3.1.2, since here we are performing regression without an intercept.) Show algebraically, and confirm numerically in R, that the t-statistic can be written as

<div align='center'>
$\frac{(\sqrt (n-1))\sum_{i=1}^n x_iy_i}{\sqrt ((\sum_{i=1}^nx_i^2)(\sum_{i^{'}=1}^n y_{i^{'}}^2)-(\sum_{i^{'}=1}^n x_{i^{'}}y_{i^{'}})^2)}$
</div>

> Equation 3.38 is given in Question 5 where,
<div align='center'>
$\hat\beta = \frac{(\sum\limits_{i=1}^n{x_iy_i})}{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}}$
</div>
<br><br>
Since t-stat without an intercept takes the form:
<div align='center'>
$\frac{\hat\beta}{SE(\hat\beta)} = \frac{\frac{\sum\limits_{i=1}^n{x_iy_i}}{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}}}{\sqrt\frac{\sum_{i=1}^n(y_i-x_i\hat\beta)^2}{(n-1)\sum_{i^{'}=1}^nx_{i^{'}}^2}} = \frac{\sum\limits_{i=1}^n{x_iy_i}}{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}} \times \frac{\sqrt{n-1}\sqrt{\sum_{i^{'}=1}^nx_{i^{'}}^2}}{\sqrt{\sum_{i=1}^n(y_i-x_i\hat\beta)^2}} = \frac{\sum\limits_{i=1}^n{x_iy_i} \times \sqrt{n-1}}{\sqrt{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}}\sqrt{\sum_{i=1}^n(y_i-x_i\hat\beta)^2}} = \frac{\sum\limits_{i=1}^n{x_iy_i} \times \sqrt{n-1}}{\sqrt{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}}\sqrt{\sum_{i=1}^n(y_i^2+x_i^2\hat\beta^2-2y_ix_i\hat\beta)}}$
</div>
<br><br>
If we then plug in the $\hat\beta$ into the denominator we get:
<div align='center'>
$\frac{\sum\limits_{i=1}^n{x_iy_i} \times \sqrt{n-1}}{\sqrt{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}}\sqrt{\sum_{i=1}^n(y_i^2+x_i^2(\frac{(\sum\limits_{i=1}^n{x_iy_i})}{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}})^2-2y_ix_i(\frac{(\sum\limits_{i=1}^n{x_iy_i})}{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}}))}} = \frac{\sqrt{n-1} \times \sum\limits_{i=1}^n{x_iy_i}}{\sqrt{(\sum_{i=1}^n x_i^2)(\sum\limits_{i^{'}=1}^{n}{y_{i^{'}}^2})-(\sum_{i^{'}}^nx_{i^{'}}y_{i{'}})}}$
</div>
<br><br>
We can verify this in R as well. n is the length of x in the first regression so:

```{r}
n <- length(x)
sqrt(n-1)*sum(x*y) / sqrt(sum(x^2)*sum(y^2)-(sum(x*y))^2)
```

> Compare this to $\frac{\hat\beta}{SE(\hat\beta)} = \frac{\frac{\sum\limits_{i=1}^n{x_iy_i}}{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}}}{\sqrt\frac{\sum_{i=1}^n(y_i-x_i\hat\beta)^2}{(n-1)\sum_{i^{'}=1}^nx_{i^{'}}^2}}$

```{r}
betahat <- sum(x*y)/sum(x^2)
SEbetahat <- sqrt(sum((y-x*betahat)^2)/((n-1)*sum(x^2)))
betahat / SEbetahat
```

> Confirming that both gives us a t-value of 18.72593.

<br><br>

## 11.(e) 
Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y.

>The equation derived in (d) for t-statistic is the same irrespective of whether we regress y onto x or x onto y. Therefore, the t-stat would be the same regardless.

<br><br>

## 11.(f) 
In R, show that when regression is performed with an intercept, the t-statistic for $H_0 : \beta = 0$ is the same for the regression of y onto x as it is for the regression of x onto y.

```{r}
elevenfy.lm <- lm(y ~ x)
elevenfx.lm <- lm(x ~ y)
summary(elevenfy.lm)
summary(elevenfx.lm)
```

> we can see that both t-values are 18.56.
<br><br>
Alternatively, we can look at the t-values of the coefficients only:

```{r}
elevenfy.lm <- lm(y ~ x)
elevenfx.lm <- lm(x ~ y)
coef(summary(elevenfy.lm))[, 't value']
coef(summary(elevenfx.lm))[, 't value']
```

<br><br><br><br><br><br>

# 12. Simple LR w/o Intercept
This problem involves simple linear regression without an intercept.

## 12.(a) 
Recall that the coefficient estimate $\hat\beta$ for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of X onto Y the same as the coefficient estimate for the regression of Y onto X?

> Since $\hat\beta_x = \frac{(\sum\limits_{i=1}^n{x_iy_i})}{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^2}}$ and $\hat\beta_y = \frac{(\sum\limits_{i=1}^n{x_iy_i})}{\sum\limits_{i^{'}=1}^{n}{y_{i^{'}}^2}}$ the two can only be equal if the denominators are equal. 
<br><br>
That is: 
<div align='center'>
$\sum\limits_{i^{'}=1}^n{x_{i^{'}}^2} = \sum\limits_{i^{'}=1}^n{y_{i^{'}}^2}$
<\div>

<br><br>

## 12.(b) 
Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X.

```{r}
set.seed(-1)
x <- rnorm(100)
y <- x^3 + rnorm(100)
twelveby.lm <- lm(y ~ x)
twelevbx.lm <- lm(x ~ y)
coef(summary(twelveby.lm))
coef(summary(twelevbx.lm))
```

<br><br>

## 12.(c) 
Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is the same as the coefficient estimate for the regression of Y onto X.

```{r}
set.seed(-1)
x <- rnorm(100)
y <- -sample(x) #sample takes a sample of the specified size from the elements of x using either with or without replacement (default is without). By choosing the sample size 100, we effectively reorder the -xs.
twelvecy.lm <- lm(y ~ x)
twelevcx.lm <- lm(x ~ y)
coef(summary(twelvecy.lm))[,'Estimate']
coef(summary(twelevcx.lm))[, 'Estimate']
```

<br><br><br><br><br><br>





# 13. Simple LR on Simulated Data
In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.

## 13.(a)
Using the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0,1) distribution. This represents a feature, X.
```{r}
set.seed(1)
x <- rnorm(100)
```

<br><br>

## 13.(b)
Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a N(0,0.25) distribution i.e. a normal distribution with mean zero and variance 0.25.
```{r}
eps <- rnorm(100, mean=0, sd=sqrt(0.25))
```

<br><br>

## 13.(c) 
Using x and eps, generate a vector y according to the model Y = −1 + 0.5X + $\varepsilon$

What is the length of the vector y? What are the values of $\beta_0$ and $\beta_1$ in this linear model?

> y = -1 + 0.5*x + eps where the length of y is 100, $\beta_0 = -1, \beta_1 = 0.5$

```{r}
y = -1 + 0.5*x + eps
```

<br><br>

## 13.(d)
Create a scatterplot displaying the relationship between x and y. Comment on what you observe.

```{r}
plot(x, y)
```

> There is a visible positive linear trend as expected.

<br><br>

## 13.(e) 
Fit a least squares linear model to predict y using x. Comment on the model obtained. How do $\hat\beta_0$ and $\hat\beta_1$ compare to $\beta_0$ and $\beta_1$?

```{r}
y.lm <- lm(y ~ x)
summary(y.lm)
```

> Both estimated values are quite close to the true values. $\hat\beta_0 = -1.01885$ is almost the same as $\beta_0 = -1$ and $\hat\beta_1 = 0.49947$ is pretty close to $\beta_1 = 0.5$

<br><br>

## 13.(f) 
Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.

```{r}
plot(x, y)
abline(y.lm, col = 'red', lwd = 2)
abline(-1, 0.5, col = 'blue', lwd = 2)
legend('topleft', legend = c('least squares','population regression'), col = c('red', 'blue'), lwd=2)
```

<br><br>

## 13.(g) 
Now fit a polynomial regression model that predicts y using x and x^2. Is there evidence that the quadratic term improves the model fit? Explain your answer.

```{r}
ypoly.lm <- lm(y ~ x + I(x^2))
summary(ypoly.lm)
```

> Although the Multiple R-squared has increased from 46.74% to 47.74%, that is likely due to the increase in the number of variables regressed. The adjusted R-square also marginally increased from 46.19% to 46.72%. However we cannot state that the quadratic term improves the model. Not least because it is not statistically significant. 

<br><br>

## 13.(h) 
Repeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term $\varepsilon$ in (b). Describe your results.

```{r}
eps <- rnorm(100, mean=0, sd=sqrt(0.25/3))
y <- -1 + 0.5*x + eps
ysmvar.lm <- lm(y ~ x)
summary(ysmvar.lm)
```

> As expected, the reduction in the random error term's variance by a third has increased the R-squared from 46.74% to 67.9%. Ie, the model can explain more of the variability in y thanks to reduction in the random noise.

<br><br>

## 13.(i) 
Repeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term ε in (b). Describe your results.

```{r}
eps <- rnorm(100, mean=0, sd=sqrt(0.25*3))
y <- -1 + 0.5*x + eps
ylgvar.lm <- lm(y ~ x)
summary(ylgvar.lm)
```

> As expected, the 3-fold increase in the random error term's variance has decreased the R-squared from 46.74% to 22.94%. Ie, the model can explain smaller portion of the variability in y thanks to a much larger random noise.

<br><br>

## 13.(j) 
What are the confidence intervals for $\beta_0$ and $\beta_1$ based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.

```{r}
confint(ysmvar.lm)
confint(y.lm)
confint(ylgvar.lm)
```

> The noisier the data, the wider the CI, and vice versa.

<br><br><br><br><br><br>



# 14. Collinearity
This problem focuses on the collinearity problem.

## 14.(a) 
Perform the following commands in R:
```{r}
set.seed(1)
x1 <- runif(100) #this generates 100 values randomly from uniform distribution 
x2 <- 0.5*x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
```
The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?

> $y = 2 + 2\times x_1 + 0.3\times x_2 + \varepsilon$
<br><br>
The regression coefficients are:

```{r}
y.lmfit <- lm(y ~ x1 + x2)
coef(y.lmfit)
```

<br><br>

## 14.(b)
What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.
```{r}
cor(x1, x2)
plot(x1, x2)
```

>There strong positive correlation between the two at 0.835, which is further evidenced by the scatterplot.

<br><br>

## 14.(c)
Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are $\hat\beta_0, \hat\beta_1$, and $\hat\beta_2$? How do these relate to the true $\beta_0, \beta_1$, and $\beta_2$? Can you reject the null hypothesis $H_0 : \beta_1 = 0$? How about the null hypothesis $H_0 : \beta_2 = 0$?
```{r}
summary(y.lmfit)
```

> Based on the F-stat of 12.8 and its p-value of 1.164e-05 we can conclude that at least one of the variables is non-zero. The regression gives us $\hat\beta_0 = 2.1305, \hat\beta_1 = 1.4396$, and $\hat\beta_2 = 1.0097$ which are not that close to $\beta_0 = 2, \beta_1 = 2$, and $\beta_2 = 0.3$. 
<br><br>
At a p-value of 0.3754 we fail to reject the null that $\beta_2 = 0$. This would also be the case for $H_0 : \beta_1 = 0$ if our cut-off point for p-value is 0.04 or below. However, we can reject the null for $\beta_1$ if our cut-off point is 0.05 or higher.

<br><br>

## 14.(d) 
Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis $H_0 : \beta_1 = 0$?
```{r}
yx1.lm <- lm(y ~ x1)
summary(yx1.lm)
```

> Although the R-squared of this regression and the previous one are roughly the same, the x1 is now statistically significant. This is expected since there is a strong collinearity between x1 and x2. Dropping x2 would not impact the R-squared that much while solving the collinearity problem. the p-value associated with $\beta_1$ is effectively 0, so we can comfortably reject the null.

<br><br>

## 14.(e) 
Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis $H_0 : \beta_1 = 0$?
```{r}
yx2.lm <- lm(y ~ x2)
summary(yx2.lm)
```

> The comments for 14.(d) also apply here. Removing one of the 2 variables that show strong collinearity is a way to resolve the issue.

<br><br>

## 14.(f)
Do the results obtained in (c)–(e) contradict each other? Explain your answer.

> They don't contradict each other. x1 and x2 show high collinearity, reducing the accuracy of the estimates of the regression coefficients. Collinearity results in larger standard errors for the coefficients, which in turn reduces the t-statistic. This in turn means a higher p-value, thus reducing the power of the hypothesis test $H_0 : \beta_j = 0$, increasing the chance of us committing a Type II error.
<br><br>
There are 2 simple solutions to deal with collinearity. One is to remove one of the variables, as we did in (d) and (e), and the other is to combine the two variables into a new varible. This is why the results are not contradictory.

<br><br>

## 14.(g)
Now suppose we obtain one additional observation, which was unfortunately mismeasured.
```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
```
Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.

> To answer this, we would have to first run the regression again and then run some diagnostic plots. 

```{r}
y14g.lm <- lm(y ~ x1 + x2)
y14gx1.lm <- lm(y ~ x1)
y14gx2.lm <- lm(y ~ x2)
summary(y14g.lm)
summary(y14gx1.lm)
summary(y14gx2.lm)
```

> An outlier may not necessarily have a strong impact on the least squares fit, especially if it doesn't have an unusual predictor value. However, it may still have an impact on RSE, which in turn can impact the CI, p-values and the R-squared. 
<br><br>
A high leverage point, on the other hand, can have a sizeable impact on the estimated regression line, as the least squares line can be heavily impacted by a couple of observations. So if there are any problems with these points, the entire fit may be invalidated. In a simple regression it is easy to observe a high leveraged point but in a multiple regression, it is possible to have an onservation that is within the range of each predictor individually, but unusual in terms of the full set of predictors.
<br><br>
In order to quantify an observation's leverage we calculate the leverage statistic:
<div align='center'>
$h_i = \frac{1}{n}+\frac{(x_i-\bar x)^2}{\sum_{i^{'}=1}^n(x_{i^{'}}-\bar x)^2}$
</div>
<br><br>
A large value of this leverage stat indicates a high leverage and vice versa. It is always between $\frac{1}{n}$ and 1, and the average leverage for all observations is always equal to $\frac{(p+1)}{n}$. So if a leverage stat is much higher than this average, we can consider that point to have a large leverage.
<br><br>
So based on the above we can see that the new mismeasured data may not be an outlier since there isn't much of an impact on the RSE, but may have a high leverage. To get a better idea we can look at the plots:

```{r}
```{r}
par(mfrow=c(1,3))
plot(x1, x2)
plot(x1, y)
plot(x2, y)
```

> Clearly the new data point is unusual when we look at both set of predictors (graph on the left) although the same point seems to be within range individually. We can also look at the residual plots:

```{r}
par(mfrow=c(2,3))
plot(predict(y14g.lm), residuals(y14g.lm))
plot(predict(y14gx1.lm), residuals(y14gx1.lm))
plot(predict(y14gx2.lm), residuals(y14gx2.lm))
plot(hatvalues(y14g.lm))
plot(hatvalues(y14gx1.lm))
plot(hatvalues(y14gx2.lm))
which.max(hatvalues(y14g.lm))
which.max(hatvalues(y14gx1.lm))
which.max(hatvalues(y14gx2.lm))
```

> Which confirms that the new data point 101 has a high leverage.

<br><br><br><br><br><br>



<h1> 15 Predicting Crime Rate in Boston </h1>
This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

<h2> 15(a) </h2> 
For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.
```{r}
library(MASS)
names(Boston)
?Boston
crimzn.lm <- lm(crim ~ zn, data = Boston)
coef(summary(crimzn.lm))[2,]
```

> Note that we could do the above for each of the 13 variables. However, to speed it up we can write a loop function instead:

```{r}
for (x in Boston[-1]) {   #Boston[-1] removes the first column which is crim.
  crimsimp.lm <- lm(crim ~ x, data = Boston) #Regress crim on predictor 1 at a time
  print(coef(summary(crimsimp.lm))[2,-c(2,3)]) #Print only the estimate and p-value
}

```

> Since we need to find out which variable is statistically significant, we can further modify the loop as:

```{r}

fifteen <- data.frame() #create a new dataframe to house our estimates and p-values from the for loop.
for (x in Boston[-1]) {   
  crimsimp.lm <- lm(crim ~ x, data = Boston) 
  estp <- coef(summary(crimsimp.lm))[2,-c(2,3)]
  fifteen <- rbind(fifteen, estp) #We put the output from the looped regression into the new dataframe
  colnames(fifteen) <- c('a.Estimates', 'P-Value') #create column names and attach these names to the new dataframe.
}

#we can then add the names of the predictors to our new data frame:
fifteen$Predictor <- (colnames(Boston)[-1])

#Alternatively, to put the predictors as the first column, 
#Predictors <- as.data.frame(colnames(Boston)[-1])
#fifteena = data.frame(Predictors = Predictors, fifteena)

#finally we can add a column to tell us whether the p value is significant:
fifteen$Significant <- c('Yes', 'No')[findInterval(fifteen$`P-Value`, c(0,0.05))]
fifteen
```
> We can see from the data frame we created that when we regress crime rates on each variable indivudally, only the proximity to Charles River (ie chas dummy) is not significant.

<br><br>

<h2> 15(b) </h2>
Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0 : \beta_j = 0$?
```{r}
bostoncrime.lm <- lm(crim ~ ., data = Boston)
summary(bostoncrime.lm)
```

> From the F-stat and its p-value we can see that at least one predictor is non-zero. We can see that the weighted mean of distances to five Boston employment centres and the index of accessibility to radial highways are statistically significant, so we can reject the null that the coefficients for these are 0. We also see that the p-value for the median value of owner-occupied homes is just above 0.001, it will be statistically significant at p-value = 0.0011 or above. The proportion of black people in town and the proportion of residential land zones for lots over 25,000 sq.ft. are only statistically significant at 0.05 cut-off. Finally, nitrogen oxide concentration and lower status of the population are statistically significant only at 0.1 cut-off.

<br><br>

<h2> 15(c) </h2>
How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.
```{r}
fifteen$b.Estimates <- coef(bostoncrime.lm)[-1]
plot(fifteen$a.Estimates, fifteen$b.Estimates)
```

<br><br>

<h2> 15(d) </h2>
Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form
<div align='center'>
$Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \varepsilon$
</div>
```{r}
#We need to modify the loop:
fifteen.d <- data.frame()
for (x in Boston[-1]) {
  crimpoly.lm <- lm(crim ~ x + I(x^2) + I(x^3), data = Boston)
  polyest <- coef(summary(crimpoly.lm))[-1,-c(2,3)]
  fifteen.d <- rbind(fifteen.d, polyest)
}
colnames(fifteen.d) <- c('d.Estimates', 'P-Value')
fifteen.d$Significant <- c('Yes','No')[findInterval(fifteen.d$`P-Value`, c(0,0.05))]
fifteen.d$Predictors <- c('zn', 'zn^2', 'zn^3','indus','indus^2','indus^3','chas','nox','nox^2','nox^3','rm','rm^2','rm^3','age','age^2','age^3','dis','dis^2','dis^3','rad','rad^2','rad^3','tax','tax^2','tax^3','ptratio','ptratio^2','ptratio^3','black','black^2','black^3','lstat','lstat^2','lstat^3','medv','medv^2','medv^3')
fifteen.d
```
> note that since chas is a dummy variable, there wouldn't be a polynomial model for it. Accordingly, we can see that there seems to be a non-linear association between the crime rate and indus, nox, age, dis, ptratio and medv.


<div ailgn = 'center'>

# [Home](https://e-usenmez.github.io/ISL/)

</div>

