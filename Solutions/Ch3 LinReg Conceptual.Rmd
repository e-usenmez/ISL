---
title: "ISL Ch 3: Linear Regression: Conceptual"
author: Emre Usenmez
date: 2021-Jan
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    number_sections: false
    theme: simplex
---

# 1. 
Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

> The null hypotheses for the p-vaules for TV, radio and newspaper are whether they are significantly different than 0. Based on the p-values we could infer that TV and radio are significantly different than 0, though we cannot reject the null hypothesis for the newspaper variable. This means, it is likely that TV and radio advertisement contribute to increase in sales, but may not necessarily be the case for newspaper advertising.

<br><br><br><br><br><br>


# 2. 
Carefully explain the differences between the KNN classifier and KNN regression methods.

> *KNN Classifier:*
<br><br>
> This is used in assessing the model accuracy for data with qualitative response. 
In the regression setting we commonly use mean squared error (MSE), whereby the MSE gets smaller as the predicted responses gets closer to the true value, thus better accuracy. Of course as a word of caution, a regression model with the lowest MSE in the training data, ie training MSE, doesn't necessarily mean the most accurate predictions on the test data. In other words, the lowest training MSE doesn't necessarily mean the lowest test MSE. (see Section 2.2 for detail).
<br><br>
Within the classifier setting (ie where the response y is not numerical but qualitative) we instead commonly use the error rate for assessing model accuracy. The Bayes classifier is the simple classifier that minimises the error rate. Unfortunately, in the real world we don't know the conditional distribution of Y given X, so computing Bayes classifier by assigning each observation to the class with the highest probability, given its predictor values, becomes impossible.
<br><br>
KNN therefore is an attempt at estimating the distribution of Y given X, and then classifying an observation to the most likely class based on its estimated probability. It does this by first identifying K number of points in the training data that are closest to a test observation (x-naught). It then looks at this group, or clusters, and estimates the conditional probability for a class as the fraction of points in the cluster. (ie given the data points in the cluster, the ratio of points that belong to a particular class is the probability that the observation belongs to that class). Finally, it assigns the test observation to the class with the highest probability in that cluster.
<br><br>
*KNN Regression Method:*
<br><br>
Linear regression is a form of parametric approach, making assumptions on the form of f(X). Whereas KNN Regression is a non-parametric method and does not make any assumptions as to the form of f(X).
<br><br>
It is similar to the KNN Classifier in that the KNN Regression first identifies K number of training points that are closest to the prediction point (x-naught). Here, in the second step, is the crucial difference. KNN Regression then estimates f(x-naught) by calculating the average of all the training responses in the cluster.
<br><br>
One way to think about it is that KNN Regression is another approach in finding an accurate model. Accordingly, if the response is quantitative, we can use MSE to assess its accuracy. 

<br><br><br><br><br><br>


# 3. 
Suppose we have a data set with five predictors, $X_1$ = GPA, $X_2$ = IQ, $X_3$ = Gender (1 for Female and 0 for Male), $X_4$ = Interaction between GPA and IQ, and $X_5$ = Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\beta_0$ = 50, $\beta_1$ = 20, $\beta_2$ = 0.07, $\beta_3$ = 35, $\beta_4$ = 0.01, $\beta_5$ = −10.

## 3.(a) 
Which answer is correct, and why?

> *iii.* For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.
<br><br>
The key to the answer lies at the interaction term X5, the coefficient of which is -10. We know that the dummy variable X3 is 1 for Female and 0 for Male. Its coefficient of 35 would mean on average a $35k higher starting salary for Female than a Male, all else being equal. However, the coefficient of -10 for an interaction term would mean a Male would begin to have a higher starting salary for GPA > 3.5.

<br><br>

## 3.(b) 
Predict the salary of a female with IQ of 110 and a GPA of 4.0.

> 50 + (20x4) + (0.07x110) + (35x1) + (0.01x4x110) + (-10x4) = 137.1 x USD 1,000 = USD 137,100

<br><br>

## 3.(c) 
True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.

> False. This statement conflates the coefficient value with significance. We would have to check the standard error of the interaction and its significance.

<br><br><br><br><br><br>


# 4. 
I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \beta_{0} + \beta_{1}X + \beta_{2}X^{2} + \beta_{3}X^{3} + \varepsilon$

## 4.(a) 
Suppose that the true relationship between X and Y is linear, i.e. $Y = \beta_{0} + \beta_{1}X + \varepsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

> Two of the most common numerical measures of model fit are the RSE and R-squared which both depend on RSS. 
<br><br>
RSS is the total sum of the squared values of the difference between each response predicted by the model and the observed response: 
<div align='center'>
$RSS = \sum_{i=1}^n(y_i-\hat y_i)^2$.
</div>
<br><br>
RSE is an estimate of the st. dev. of the error term and calculated by dividing the RSS by (n-p-1) and taking the sq. root: 
<div align='center'>
$RSE = \sqrt{\frac{RSS}{n-p-1}}$
</div>
<br><br>
R-squared, on the other hand, is a proportion that tells us how much of the variation is explained by the model, calculated by dividing the difference between TSS and RSS by the total sum of squares (TSS):
<div align='center'>
$R^2 = \frac{TSS-RSS}{TSS}$
</div>
<br><br>
TSS measures the total variance in the response Y, and is effectively the variability in the response before the regression is performed (as opposed to RSS which is the unexplained variability after the regression). It is calculated by summing the squares of the difference between each response point and the mean response: 
<div align='center'>
$TSS = \sum(y_i-\bar y)^2$
</div>
<br><br>
We know the RSS formula for the linear model is:
<br><br>
<div align='center'>
$RSS_{lm} = \sum\limits_{i=1}^n{(y_i-\bar y_i)^2}  = \sum\limits_{i=1}^n{[y_i-(\beta_0+\beta_1x_i)^2]}$
</div>
<br><br>
and the RSS for the polynomial model is:
<br><br>
<div align='center'>
$RSS_{pm} = \sum\limits_{i=1}^n{[y_i-(\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_2x_i^3)^2]}$
</div>
<br><br>
So if $\beta_2$ and $\beta_3$ are 0, then $RSS_{lm} = RSS_{pm}$, and if they are non-zero $RSS_{lm} > RSS_{pm}$. Therefore the polynomial model's RSS must be smaller than or equal to the RSS of the linear model.

<br><br>

## 4.(b) 
Answer (a) using test rather than training RSS.

> This alludes to the difference between training and test. Even though the RSS for the polynomial model will be lower in the training set, the linear model will have a lower RSS in the test set, given the true relationship is linear. This would be an example of the polynomial model over-fitting in the training data set.

<br><br>

## 4.(c) 
Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

> Again, mathematically, the RSS for the polynomial model would be smaller for the training data set irrespective of the shape of the relationship between X and Y. 

<br><br>

## 4.(d) 
Answer (c) using test rather than training RSS.

> Since this is the test rather than training, we would expect the polynomial model to be a better fit (ie lower RSS) if the true model is quite non-linear. However, if the true non-linearity is slight, then the linear model may end up having a smaller RSS. 

<br><br><br><br><br><br>


# 5. 
Consider the fitted values that result from performing linear regres- sion without an intercept. In this setting, the ith fitted value takes the form 
<div align='center'>
$\hat{y} = x_{i}\hat\beta$
</div> 
where
<div align='center'>
$\hat\beta = \frac{(\sum\limits_{i=1}^n{x_{i}y_i})}{\sum\limits_{i^{'}=1}^{n}{x_{i^{'}}^{2}}}$
</div>
Show that we can write
<div align='center'>
$\hat{y_{i}} = \sum\limits_{i^{'}=1}^n{a_{i^{'}}y_{i^{'}}}$
</div>
What is $a_{i^{'}}$?

*Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.*

> We can rewrite the first equation as:
<br><br>
<div align='center'>
$\hat y_i = x_i \frac{(\sum\limits_{i=1}^n{x_iy_i})}{\sum\limits_{i^{'}=1}^n x_{i^{'}}^2}$
</div>
<br><br>
Since $x_i$ is constant, we can take it inside the summation and relabel the others with the tick mark to distinguish them from this constant (you can instead do j,k subscript as well):
<br><br>
<div align='center'>
$\hat y_i = \frac{(\sum\limits_{i^{'}=1}^n{x_ix_{i^{'}}y_{i^{'}}})}{\sum\limits_{i^{''}=1}^{n}{x_{i^{''}}^2}}$
</div>
<br><br>
For clarity, we can rearrange and take $y_{i^{'}}$ outside:
<br><br>
<div align='center'>
$\hat y_i = \sum\limits_{i^{'}=1}^n\frac{({x_{i}x_{i^{'}}})}{\sum\limits_{i^{''}=1}^{n}{x_{i^{''}}^{2}}}y_{i^{'}} = \sum\limits_{i^{'}=1}^na_i{'}y_{i^{'}}$
</div>
<br><br>
which would make it obvious then that
<div align='center'>
$a_{i^{'}}=\frac{({x_{i}x_{i^{'}}})}{\sum\limits_{i^{''}=1}^{n}{x_{i^{''}}^{2}}}$
</div>
<br><br>
Since the interpretation of this result is that the fitted values from linear regression are linear combinations of response values, $a_{i^{'}}$ can be thought of as a scalar, a weight dependent on x for each observation associated with each response. 

<br><br><br><br><br><br>


# 6. 
Using the following:

<div align='center'>
$\hat\beta_0 = \bar y - \hat\beta_1\bar x$
</div>
and
<div align='center'>
$\hat\beta_1 = \frac{\sum_{i=1}^n (x_{i}-\bar x)(y_{i}-\bar y)}{\sum_{i=1}^n (x_{i} - \bar x)^2}$
</div>
argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar x, \bar y)$

> To answer this question we would have to demonstrate when x = $\bar x$ then $\hat y = \bar y$ If this is the case, then it is true that the least squares line always passes through this point in the case of simple linear regression. 
<br><br>
We know the simple linear regression is $\hat y = \beta_0 + \beta_1 x$   
So if x = $\bar x$ then the equation becomes: $\hat y = \beta_0 + \beta_1 \bar x$
<br><br>
if we then substitute $\beta_0$ we get:
$\hat y = (\bar y - \hat\beta_1\bar x) + \beta_1 \bar x$
which simplifies to: $\hat y = \bar y$ thus proving that the line always passes through the point $(\bar x, \bar y)$

<br><br><br><br><br><br>


# 7. 

It is claimed in the text that in the case of simple linear regression of Y onto X, the R-squared statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that $\bar x = \bar y = 0$.

> The two equations are:
<br><br>
<div align='center'>
$R^2 = \frac {TSS - RSS}{TSS} = 1 - \frac {RSS}{TSS}$
</div>
<br><br>
and
<br><br>
<div align='center'>
$Cor(x,y) = \frac{\sum\limits_{i=1}^n{(x_{i}-\bar x)(y_{i}-\bar y)}}{\sqrt{\sum\limits_{i=1}^n{(x_{i}-\bar x)^2}}\sqrt{\sum\limits_{i=1}^n{(y_{i}-\bar y)^2}}}$
</div>
<br><br>
we also know $TSS = \sum{(y_{i}-\bar y)^2}$ and $RSS = \sum\limits_{i=1}^n{(y_{i}-\hat y_{i})^2}$
<br><br>
and if $\bar x = \bar y = 0$   then   $TSS = \sum\limits_{i=1}^n{y^2_{i}}$   and   $Cor(x, y) = \frac {\sum\limits_{i=1}^n (x_iy_i)}{\sum\limits_{i=1}^n x_i\sum\limits_{i=1}^n y_i}$
<br><br>
So R-squared becomes:
<br><br>
<div align='center'>
$R^2 =\frac{\sum\limits_{i=1}^n{y^2_{i}}-\sum\limits_{i=1}^n{(y_{i}-\hat y_{i})^2}}{\sum\limits_{i=1}^n{y^2_{i}}}=\frac{\sum\limits_{i=1}^n{y^2_{i}}-{(y^2_{i}+\hat y^2_{i}-2y_{i}\hat y_{i})}}{\sum\limits_{i=1}^n{y^2_{i}}}=\frac{\sum\limits_{i=1}^n{(-\hat y^2_{i}+2y_{i}\hat y_{i})}}{\sum\limits_{i=1}^n{y^2_{i}}}$
</div>
<br><br>
A simple linear regression is $y=\beta_0 + \beta_1x$ and for the line to pass through $\bar x = \bar y = 0$ then $\hat\beta_0 = \bar y - \hat\beta_1\bar x$ must equal to 0, thus the regression equation becomes $\hat y = \hat\beta_1 x$
<br><br>
We can then substitute this into the R-squared equation above:
<br><br>
<div align='center'>
$R^2 = \frac {\sum\limits_{i=1}^n{-(\hat\beta_1 x_{i})^2+2y_{i}\hat\beta_1 x_{i} }}{\sum\limits_{i=1}^n{y^2_{i}}} = \frac {\hat\beta_1(\sum\limits_{i=1}^n{-\hat\beta_1 x_{i}^2+2y_{i} x_{i}})}{\sum\limits_{i=1}^n{y^2_{i}}}$
</div>
<br><br>
We additionally know that
<br><br>
<div align='center'>
$\hat\beta_1 = \frac {\sum\limits_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum\limits_{i=1}^n(x_i-\bar x)^2}$ 
</div>
and since $\bar x = \bar y = 0$ this simplifies to: 
<br><br>
><div align='center'>
$\hat\beta_1 = \frac {\sum\limits_{i=1}^n x_iy_i}{\sum\limits_{i=1}^n x_i^2}$
</div>
<br><br>
We can then substitute this into the R-squared equation:
<br><br>
<div align='center'>
$R^2 = \frac {\sum\limits_{i=1}^n x_iy_i}{\sum\limits_{i=1}^n x_i^2} \frac {\sum\limits_{i=1}^n{2y_{i} x_{i}-x_{i}^2\frac {\sum\limits_{i=1}^n x_iy_i}{\sum\limits_{i=1}^n x_i^2}}}{\sum\limits_{i=1}^n{y^2_{i}}} = \frac {\sum\limits_{i=1}^n x_iy_i}{\sum\limits_{i=1}^n x_i^2} \frac{\sum\limits_{i=1}^n{}2y_i x_i - x_i y_i}{\sum\limits_{i=1}^n{y^2_i}} = \frac {\sum\limits_{i=1}^n (x_iy_i)^2}{\sum\limits_{i=1}^n x_i^2\sum\limits_{i=1}^n y_i^2} = (\frac {\sum\limits_{i=1}^n (x_iy_i)}{\sum\limits_{i=1}^n x_i\sum\limits_{i=1}^n y_i})^2 = Cor(x, y)^2$
</div>

<br><br><br><br><br><br>

# [Home](https://e-usenmez.github.io/ISL/)